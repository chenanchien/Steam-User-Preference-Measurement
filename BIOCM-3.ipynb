{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 800,
   "id": "9d9975f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "from sklearn import datasets\n",
    "import csv\n",
    "import warnings\n",
    "from statistics import NormalDist\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "id": "8df70d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3494282   4.3697517  -0.26038707 ... -0.30490764 -0.3494282\n",
      " -0.30490764]\n"
     ]
    }
   ],
   "source": [
    "x_train=list()\n",
    "y_train=list()\n",
    "z=list()\n",
    "with open('/Users/chengyu/Downloads/20220108_sentiment_feature_fixed.csv', newline='') as csvfile:\n",
    "     # 讀取 CSV 檔內容，將每一列轉成一個 dictionary\n",
    "    rows = csv.DictReader(csvfile)\n",
    "\n",
    "    # 以迴圈輸出指定欄位\n",
    "    for row in rows:\n",
    "        x_train.append([row['gun_pos'],row['gun_neg'],\n",
    "                        row['teamwork_pos'],row['teamwork_neg'],\n",
    "                        row['loot_pos'],row['loot_neg'],\n",
    "                        row['price_pos'],row['price_neg'],\n",
    "                        row['cheat_pos'],row['cheat_neg'],\n",
    "                        row['optimization_pos'],row['optimization_neg'],\n",
    "                        row['graphic_pos'],row['graphic_neg']])\n",
    "        y_train.append(row['recommended'])\n",
    "\n",
    "with open('/Users/chengyu/Downloads/20220108_sentiment_feature_final.csv', newline='') as csvfile:\n",
    "     # 讀取 CSV 檔內容，將每一列轉成一個 dictionary\n",
    "    rows = csv.DictReader(csvfile)\n",
    "    for row in rows:\n",
    "        z.append(row['auther.num_reivews'])\n",
    "z = np.array(z,dtype=float)\n",
    "z=(z - np.mean(z)) / np.std(z)\n",
    "print(z)\n",
    "x_train =np.array(x_train,dtype=float)\n",
    "y_train = np.array(y_train,dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "id": "e8b47bad-a2c0-4568-96c5-f0042abdb346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_one(a):\n",
    "    return a.reshape((len(X),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "id": "37bb3428",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbitRegression:\n",
    "\n",
    "    def __init__(self, y, X, beta, z, gamma=0):\n",
    "        self.X, self.y, self.beta, self.z = X, y, beta, z #beta是係數 z為評論：為每個評論的作者的總評論數\n",
    "        self.n, self.k = X.shape\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def exp(self):\n",
    "        return np.exp(-1*self.z*self.gamma)\n",
    "    def cdf(self):  # cumulative distribution function\n",
    "        exp=self.exp()\n",
    "        return  norm.cdf(self.X.dot(self.beta) * exp)# @是矩陣相乘\n",
    "\n",
    "    def pdf(self):  # probability density function\n",
    "        exp=self.exp()\n",
    "        #print(self.X.dot(self.beta).shape)\n",
    "        return norm.pdf(self.X.dot(self.beta) * exp)\n",
    "\n",
    "    def logL(self): #loss\n",
    "        cdf = self.cdf()\n",
    "        return np.sum(self.y * np.log(cdf) + (1 - self.y) * np.log(1 - cdf))    \n",
    "\n",
    "    def G(self):  # gradient\n",
    "        X = self.X\n",
    "        y = self.y\n",
    "        exp = self.exp()\n",
    "        gamma = self.gamma\n",
    "        z = self.z\n",
    "        beta = self.beta\n",
    "        cdf = self.cdf()\n",
    "        pdf = self.pdf()\n",
    "        print(np.where(cdf == 1)[0])\n",
    "        #you will get all beta j's data value since Differential\n",
    "        return np.append(\n",
    "                        np.sum(\n",
    "                                y * ((X.T*pdf*exp) / cdf) + \n",
    "                                (1-y) * (-1) * ((X.T*(-pdf)*exp) / (1-cdf))\n",
    "                                ,axis=1\n",
    "                              ), \n",
    "                        np.sum(\n",
    "                               y * ((pdf*exp*(X.dot(beta))*-z)/cdf) +\n",
    "                               (1-y) * (-1) * (pdf*exp*(X.dot(beta))*(-z)/(1-cdf))\n",
    "                               , axis=0\n",
    "                              )\n",
    "                        )\n",
    "    \n",
    "\n",
    "    def H(self): #hessian\n",
    "        X = self.X\n",
    "        y=self.y\n",
    "        exp = self.exp()\n",
    "        gamma = self.gamma\n",
    "        z = self.z\n",
    "        beta = self.beta\n",
    "        cdf = self.cdf()\n",
    "        pdf = self.pdf()\n",
    "        #print(X.T.shape)\n",
    "        H11=list() #14*14\n",
    "        H01=list() #14*1\n",
    "        H10=list() #1*14\n",
    "        H00=list() #1*1\n",
    "        for i in range(X.T.shape[0]): \n",
    "            temp = list()\n",
    "            for j in range (X.T.shape[0]):\n",
    "                temp.append(\n",
    "                            np.sum(\n",
    "                                    y *(X.T[i]*exp)*(X.T[j]*exp)*((-1*pdf*pdf)/(cdf*cdf)- (pdf*(X.dot(beta)*exp))/cdf)+\n",
    "                                    (1-y)*(-1)*(X.T[i]*exp)*(X.T[j]*exp)*((pdf*pdf)/(1-(cdf*cdf)) - (pdf*(X.dot(beta)*exp))/(1-cdf))\n",
    "                                  )\n",
    "                            )\n",
    "            H11.append(temp)\n",
    "        \n",
    "        for i in range(X.T.shape[0]): \n",
    "            H01.append(\n",
    "                       np.sum(\n",
    "                                y*(-1*exp*z)*(((pdf/cdf)*X.T[i])+(X.T[i]*exp)*((-1*pdf*pdf)/(cdf*cdf) - (pdf*(X.dot(beta)*exp))/cdf))+\n",
    "                                (1-y)*(exp*z)*(((pdf/(1-cdf))*X.T[i])+(X.T[i]*exp)*((pdf*pdf)/(1-(cdf*cdf)) - (pdf*(X.dot(beta)*exp))/(1-cdf)))\n",
    "                             )\n",
    "                      )\n",
    "        for i in range(X.T.shape[0]): \n",
    "            H10.append(\n",
    "                       np.sum(\n",
    "                               y *X.T[i] *(-1*exp*z)*((pdf/cdf)-((X.dot(beta)*exp)*((pdf*pdf)/(cdf*cdf) + (pdf/cdf)*(X.dot(beta)*exp))))+\n",
    "                               (1-y) * (-1*X.T[i]) * (-1*exp*z)* ((pdf/(1-cdf)) + ((X.dot(beta)*exp)*((pdf*pdf)/(cdf*cdf) + (pdf/cdf)*(X.dot(beta)*exp))))\n",
    "                             )\n",
    "                    )\n",
    "        H00.append(\n",
    "                    np.sum(\n",
    "                            y*(-1*z*X.dot(beta)) * (-1*exp*z)*((pdf/cdf)-((X.dot(beta)*exp)*((pdf*pdf)/(cdf*cdf) + (pdf/cdf)*(X.dot(beta)*exp)))) +\n",
    "                            (1-y)*(z*X.dot(beta)) * (-1*exp*z)* ((pdf/(1-cdf)) + ((X.dot(beta)*exp)*((pdf*pdf)/(cdf*cdf) + (pdf/cdf)*(X.dot(beta)*exp))))\n",
    "                          )\n",
    "                  )\n",
    "\n",
    "        H11=np.array(H11)\n",
    "        H01=np.array(H01).reshape(1,14)\n",
    "        H10=np.array(H10).reshape(1,14)\n",
    "        H00=np.array(H00).reshape(1,1)\n",
    "        H = np.append(H11,H01,axis=0)\n",
    "        temp_H = np.append(H10,H00).reshape(1,15)\n",
    "        H = np.append(H,temp_H.T,axis=1)\n",
    "        \n",
    "        return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "id": "1a2d484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_raphson(model, tol=1e-3, max_iter=1000, display=True):\n",
    "\n",
    "    i = 0\n",
    "    error = 100  # Initial error value\n",
    "\n",
    "    # Print header of output\n",
    "    if display:\n",
    "        header = f'{\"Iteration_k\":<13}{\"Log-likelihood\":<16}{\"θ\":<60}'\n",
    "        print(header)\n",
    "        print(\"-\" * len(header))\n",
    "\n",
    "    # While loop runs while any value in error is greater\n",
    "    # than the tolerance until max iterations are reached\n",
    "    while np.any(error > tol) and i < max_iter:\n",
    "        H, G = model.H(), model.G()\n",
    "        #print(H)\n",
    "        #print(G)\n",
    "        print('before')\n",
    "        print(model.beta)\n",
    "        print(model.gamma)\n",
    "        param = np.append(model.beta,model.gamma)\n",
    "        \n",
    "        \n",
    "        beta_new = param - (np.linalg.inv(H).dot(G))\n",
    "        error = np.linalg.norm(beta_new - param)\n",
    "        model.beta = np.array(beta_new[:14])\n",
    "        model.gamma = np.array(beta_new[-1])\n",
    "        print('after')\n",
    "        print(model.beta)\n",
    "        print(model.gamma)\n",
    "        # Print iterations\n",
    "        if display:\n",
    "            beta_list = [f'{t:.3}' for t in list(model.beta.flatten())]\n",
    "            update = f'{i:<13}{model.logL():<16.8}{beta_list}'\n",
    "            #print(update)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    print(f'Number of iterations: {i}')\n",
    "    print(f'β_hat = {model.beta.flatten()}')\n",
    "\n",
    "    # Return a flat array for β (instead of a k_by_1 column vector)\n",
    "    return model.beta,model.gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "id": "f2e50bb8-2cdd-4e7b-8390-76f1388ca9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProbitRegression(y_train, x_train, np.zeros(len(x_train[0])),z,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "id": "a5c0ad62-c29e-48cc-aae6-e00c21298371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration_k  Log-likelihood  θ                                                           \n",
      "-----------------------------------------------------------------------------------------\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "[]\n",
      "before\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "1\n",
      "after\n",
      "[ 1.80078727  0.34804575  2.14644351  0.45175476  0.49637737  0.15492852\n",
      "  1.65560066 -1.65137947  1.18600136 -0.46598375  1.50372907  0.24801116\n",
      "  1.60006162  0.26022964]\n",
      "12.070333617158573\n",
      "[ 0.15492852  0.34804575  2.0610169   0.5082408   2.14644351  0.\n",
      "  0.         -0.46598375 -0.46598375 -0.46598375  1.65560066  0.24801116\n",
      " -1.65137947  0.          2.14644351 -0.46598375  0.34804575 -0.46598375\n",
      " -0.46598375  2.14883302  0.24801116  0.26022964  0.          0.15492852\n",
      " -0.46598375  1.80078727 -0.46598375  1.13407787  0.24801116  0.34804575\n",
      "  4.54010267 -0.46598375  3.59134971  0.         -0.46598375  0.24801116\n",
      " -0.46598375  0.24801116 -0.21797258  1.33480352 -1.65137947 -0.21797258\n",
      " -0.46598375  3.88168393 -1.40336831  0.34804575  0.59605692  1.68045976\n",
      "  0.24801116  0.69976592  2.14644351 -0.21797258 -2.11736322  0.30433632\n",
      "  1.68897563 -0.11793799 -0.46598375 -1.65137947  0.50297428 -2.11736322\n",
      " -0.46598375  0.          0.34804575  0.          0.         -0.46598375\n",
      " -0.46598375 -0.46598375  0.24801116 -0.46598375  0.         -0.46598375\n",
      " -0.11793799  0.24801116 -1.65137947 -0.20575411  0.          1.60006162\n",
      "  0.34804575  1.80078727  1.80078727  0.         -0.46598375  1.80078727\n",
      "  0.34804575  0.24801116  2.14644351  1.18600136 -0.46598375 -1.40336831\n",
      " -2.11736322  2.14644351  4.90457796 -0.21797258  1.48212363  2.14644351\n",
      "  1.80078727  0.         -0.46598375  0.34804575]\n",
      "[ 0.15492852  0.34804575  2.0610169   0.5082408   2.14644351  0.\n",
      "  0.         -0.46598375 -0.46598375 -0.46598375  1.65560066  0.24801116\n",
      " -1.65137947  0.          2.14644351 -0.46598375  0.34804575 -0.46598375\n",
      " -0.46598375  2.14883302  0.24801116  0.26022964  0.          0.15492852\n",
      " -0.46598375  1.80078727 -0.46598375  1.13407787  0.24801116  0.34804575\n",
      "  4.54010267 -0.46598375  3.59134971  0.         -0.46598375  0.24801116\n",
      " -0.46598375  0.24801116 -0.21797258  1.33480352 -1.65137947 -0.21797258\n",
      " -0.46598375  3.88168393 -1.40336831  0.34804575  0.59605692  1.68045976\n",
      "  0.24801116  0.69976592  2.14644351 -0.21797258 -2.11736322  0.30433632\n",
      "  1.68897563 -0.11793799 -0.46598375 -1.65137947  0.50297428 -2.11736322\n",
      " -0.46598375  0.          0.34804575  0.          0.         -0.46598375\n",
      " -0.46598375 -0.46598375  0.24801116 -0.46598375  0.         -0.46598375\n",
      " -0.11793799  0.24801116 -1.65137947 -0.20575411  0.          1.60006162\n",
      "  0.34804575  1.80078727  1.80078727  0.         -0.46598375  1.80078727\n",
      "  0.34804575  0.24801116  2.14644351  1.18600136 -0.46598375 -1.40336831\n",
      " -2.11736322  2.14644351  4.90457796 -0.21797258  1.48212363  2.14644351\n",
      "  1.80078727  0.         -0.46598375  0.34804575]\n",
      "[ 0.15492852  0.34804575  2.0610169   0.5082408   2.14644351  0.\n",
      "  0.         -0.46598375 -0.46598375 -0.46598375  1.65560066  0.24801116\n",
      " -1.65137947  0.          2.14644351 -0.46598375  0.34804575 -0.46598375\n",
      " -0.46598375  2.14883302  0.24801116  0.26022964  0.          0.15492852\n",
      " -0.46598375  1.80078727 -0.46598375  1.13407787  0.24801116  0.34804575\n",
      "  4.54010267 -0.46598375  3.59134971  0.         -0.46598375  0.24801116\n",
      " -0.46598375  0.24801116 -0.21797258  1.33480352 -1.65137947 -0.21797258\n",
      " -0.46598375  3.88168393 -1.40336831  0.34804575  0.59605692  1.68045976\n",
      "  0.24801116  0.69976592  2.14644351 -0.21797258 -2.11736322  0.30433632\n",
      "  1.68897563 -0.11793799 -0.46598375 -1.65137947  0.50297428 -2.11736322\n",
      " -0.46598375  0.          0.34804575  0.          0.         -0.46598375\n",
      " -0.46598375 -0.46598375  0.24801116 -0.46598375  0.         -0.46598375\n",
      " -0.11793799  0.24801116 -1.65137947 -0.20575411  0.          1.60006162\n",
      "  0.34804575  1.80078727  1.80078727  0.         -0.46598375  1.80078727\n",
      "  0.34804575  0.24801116  2.14644351  1.18600136 -0.46598375 -1.40336831\n",
      " -2.11736322  2.14644351  4.90457796 -0.21797258  1.48212363  2.14644351\n",
      "  1.80078727  0.         -0.46598375  0.34804575]\n",
      "[     0      2      3 ... 125070 125073 125075]\n",
      "before\n",
      "[ 1.80078727  0.34804575  2.14644351  0.45175476  0.49637737  0.15492852\n",
      "  1.65560066 -1.65137947  1.18600136 -0.46598375  1.50372907  0.24801116\n",
      "  1.60006162  0.26022964]\n",
      "12.070333617158573\n",
      "after\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "nan\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan]\n",
      "Number of iterations: 2\n",
      "β_hat = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan]),\n",
       " array(nan))"
      ]
     },
     "execution_count": 865,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!!!!!!!!\n",
    "newton_raphson(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "id": "085bf0c6-73e7-4aa1-a068-aaf41f2922b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "(15,)\n"
     ]
    }
   ],
   "source": [
    "print(model.G().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184c2f1a-c094-4e52-b73d-88e50b53295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.H())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5cce88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
